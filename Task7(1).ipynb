{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abney ... ORG\n",
      "1991 ... DATE\n",
      "Steven Abney ... PERSON\n",
      "Ph.D. ... WORK_OF_ART\n",
      "MIT ... ORG\n",
      "1987 ... DATE\n",
      "the University of Michigan ... ORG\n",
      "Abney ... PERSON\n",
      "' PUNCT ``\n",
      "In ADP IN\n",
      "natural ADJ JJ\n",
      "language NOUN NN\n",
      "processing NOUN NN\n",
      ", PUNCT ,\n",
      "chinking VERB VBG\n",
      "is AUX VBZ\n",
      "a DET DT\n",
      "technique NOUN NN\n",
      "used VERB VBN\n",
      "in ADP IN\n",
      "information NOUN NN\n",
      "extraction NOUN NN\n",
      "to PART TO\n",
      "remove VERB VB\n",
      "parts NOUN NNS\n",
      "of ADP IN\n",
      "a DET DT\n",
      "chunk NOUN NN\n",
      "that PRON WDT\n",
      "do AUX VBP\n",
      "not PART RB\n",
      "fit VERB VB\n",
      "a DET DT\n",
      "specific ADJ JJ\n",
      "pattern NOUN NN\n",
      ". PUNCT .\n",
      "It PRON PRP\n",
      "is AUX VBZ\n",
      "essentially ADV RB\n",
      "the DET DT\n",
      "inverse NOUN NN\n",
      "of ADP IN\n",
      "the DET DT\n",
      "process NOUN NN\n",
      "of ADP IN\n",
      "chunking NOUN NN\n",
      ", PUNCT ,\n",
      "which PRON WDT\n",
      "involves VERB VBZ\n",
      "identifying NOUN NN\n",
      "and CCONJ CC\n",
      "grouping VERB VBG\n",
      "together ADV RB\n",
      "parts NOUN NNS\n",
      "of ADP IN\n",
      "a DET DT\n",
      "sentence NOUN NN\n",
      "that PRON WDT\n",
      "belong VERB VBP\n",
      "to ADP IN\n",
      "a DET DT\n",
      "specific ADJ JJ\n",
      "syntactic ADJ JJ\n",
      "category NOUN NN\n",
      ". PUNCT .\n",
      "While SCONJ IN\n",
      "chunking VERB VBG\n",
      "is AUX VBZ\n",
      "useful ADJ JJ\n",
      "for ADP IN\n",
      "identifying VERB VBG\n",
      "meaningful ADJ JJ\n",
      "chunks NOUN NNS\n",
      "of ADP IN\n",
      "text NOUN NN\n",
      ", PUNCT ,\n",
      "chinking VERB VBG\n",
      "can AUX MD\n",
      "be AUX VB\n",
      "used VERB VBN\n",
      "to PART TO\n",
      "exclude VERB VB\n",
      "parts NOUN NNS\n",
      "of ADP IN\n",
      "a DET DT\n",
      "chunk NOUN NN\n",
      "that PRON WDT\n",
      "are AUX VBP\n",
      "not PART RB\n",
      "relevant ADJ JJ\n",
      "to ADP IN\n",
      "the DET DT\n",
      "analysis NOUN NN\n",
      ". PUNCT .\n",
      "Chinking VERB VBG\n",
      "was AUX VBD\n",
      "first ADV RB\n",
      "introduced VERB VBN\n",
      "by ADP IN\n",
      "Abney PROPN NNP\n",
      "in ADP IN\n",
      "1991 NUM CD\n",
      "as ADP IN\n",
      "part NOUN NN\n",
      "of ADP IN\n",
      "his PRON PRP$\n",
      "work NOUN NN\n",
      "on ADP IN\n",
      "natural ADJ JJ\n",
      "language NOUN NN\n",
      "parsing VERB VBG\n",
      ". PUNCT .\n",
      "Steven PROPN NNP\n",
      "Abney PROPN NNP\n",
      "is AUX VBZ\n",
      "a DET DT\n",
      "computational ADJ JJ\n",
      "linguist NOUN NN\n",
      "who PRON WP\n",
      "has AUX VBZ\n",
      "made VERB VBN\n",
      "significant ADJ JJ\n",
      "contributions NOUN NNS\n",
      "to ADP IN\n",
      "the DET DT\n",
      "field NOUN NN\n",
      "of ADP IN\n",
      "computational ADJ JJ\n",
      "linguistics NOUN NNS\n",
      "and CCONJ CC\n",
      "natural ADJ JJ\n",
      "language NOUN NN\n",
      "processing NOUN NN\n",
      ". PUNCT .\n",
      "He PRON PRP\n",
      "received VERB VBD\n",
      "his PRON PRP$\n",
      "Ph.D. NOUN NN\n",
      "in ADP IN\n",
      "Linguistics PROPN NNP\n",
      "from ADP IN\n",
      "MIT PROPN NNP\n",
      "in ADP IN\n",
      "1987 NUM CD\n",
      "and CCONJ CC\n",
      "is AUX VBZ\n",
      "currently ADV RB\n",
      "a DET DT\n",
      "researcher NOUN NN\n",
      "at ADP IN\n",
      "the DET DT\n",
      "University PROPN NNP\n",
      "of ADP IN\n",
      "Michigan PROPN NNP\n",
      ". PUNCT .\n",
      "Abney PROPN NNP\n",
      "'s PART POS\n",
      "work NOUN NN\n",
      "on ADP IN\n",
      "chinking VERB VBG\n",
      "helped VERB VBD\n",
      "to PART TO\n",
      "improve VERB VB\n",
      "the DET DT\n",
      "accuracy NOUN NN\n",
      "of ADP IN\n",
      "natural ADJ JJ\n",
      "language NOUN NN\n",
      "parsing VERB VBG\n",
      "by ADP IN\n",
      "allowing VERB VBG\n",
      "for ADP IN\n",
      "more ADV RBR\n",
      "precise ADJ JJ\n",
      "identification NOUN NN\n",
      "of ADP IN\n",
      "phrases NOUN NNS\n",
      "and CCONJ CC\n",
      "their PRON PRP$\n",
      "parts NOUN NNS\n",
      ". PUNCT .\n",
      "The DET DT\n",
      "technique NOUN NN\n",
      "has AUX VBZ\n",
      "since SCONJ IN\n",
      "become VERB VBN\n",
      "a DET DT\n",
      "standard ADJ JJ\n",
      "tool NOUN NN\n",
      "in ADP IN\n",
      "natural ADJ JJ\n",
      "language NOUN NN\n",
      "processing NOUN NN\n",
      "and CCONJ CC\n",
      "is AUX VBZ\n",
      "widely ADV RB\n",
      "used VERB VBN\n",
      "in ADP IN\n",
      "various ADJ JJ\n",
      "applications NOUN NNS\n",
      ", PUNCT ,\n",
      "including VERB VBG\n",
      "sentiment NOUN NN\n",
      "analysis NOUN NN\n",
      ", PUNCT ,\n",
      "named VERB VBN\n",
      "entity NOUN NN\n",
      "recognition NOUN NN\n",
      ", PUNCT ,\n",
      "and CCONJ CC\n",
      "machine NOUN NN\n",
      "translation NOUN NN\n"
     ]
    }
   ],
   "source": [
    "example=\"'In natural language processing, chinking is a technique used in information extraction to remove parts of a chunk that do not fit a specific pattern. It is essentially the inverse of the process of chunking, which involves identifying and grouping together parts of a sentence that belong to a specific syntactic category. While chunking is useful for identifying meaningful chunks of text, chinking can be used to exclude parts of a chunk that are not relevant to the analysis.Chinking was first introduced by Abney in 1991 as part of his work on natural language parsing. Steven Abney is a computational linguist who has made significant contributions to the field of computational linguistics and natural language processing. He received his Ph.D. in Linguistics from MIT in 1987 and is currently a researcher at the University of Michigan.Abney's work on chinking helped to improve the accuracy of natural language parsing by allowing for more precise identification of phrases and their parts. The technique has since become a standard tool in natural language processing and is widely used in various applications, including sentiment analysis, named entity recognition, and machine translation\"\n",
    "doc = m(example)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(\"{0} ... {1}\".format(ent.text,ent.label_))\n",
    "\n",
    "for token in doc:  \n",
    "    # Print each token\n",
    "    print(token, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abney ORG\n",
      "1991 DATE\n",
      "Steven Abney PERSON\n",
      "Ph.D. WORK_OF_ART\n",
      "MIT ORG\n",
      "1987 DATE\n",
      "the University of Michigan ORG\n",
      "Abney PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "\n",
    "    # Print the named entity and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentimental Analysis of IDMB reviews\n",
    "#imoprt libraries \n",
    "import pandas as pd\n",
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the text data:\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags and non-alphanumeric characters\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    \n",
    "    # Convert to lowercase and tokenize\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stop words and lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join tokens into a string\n",
    "    return ' '.join(tokens)\n",
    "    \n",
    "df['review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the data into training and testing sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the text data using the tf-idf vectorizer:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train a classifier (e.g., logistic regression) on the vectorized data:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the classifier on the testing set:\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model using the pickle module in Python:\n",
    "import pickle\n",
    "\n",
    "# Save the vectorizer and classifier\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump((vectorizer, clf), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved vectorizer and classifier\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    vectorizer, clf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
